{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "from scipy.io import savemat\n",
    "import numpy as np\n",
    "import itertools\n",
    "from surgeon_recording.reader import Reader\n",
    "from scipy.io import savemat\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datafolder = join('..', 'data', 'cuts')\n",
    "datafolder = join('F:\\PDM\\data','cuts') #data sur disque dur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_files = 20\n",
    "exp_folders = [join(datafolder, 'cut' + str(i)) for i in (np.arange(nb_files) + 2)]   #prend les vaeurs datafolder/cuti avec i de 2 à 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F:\\\\PDM\\\\data\\\\cuts\\\\cut2',\n",
       " 'F:\\\\PDM\\\\data\\\\cuts\\\\cut3',\n",
       " 'F:\\\\PDM\\\\data\\\\cuts\\\\cut4',\n",
       " 'F:\\\\PDM\\\\data\\\\cuts\\\\cut5',\n",
       " 'F:\\\\PDM\\\\data\\\\cuts\\\\cut6',\n",
       " 'F:\\\\PDM\\\\data\\\\cuts\\\\cut7',\n",
       " 'F:\\\\PDM\\\\data\\\\cuts\\\\cut8',\n",
       " 'F:\\\\PDM\\\\data\\\\cuts\\\\cut9',\n",
       " 'F:\\\\PDM\\\\data\\\\cuts\\\\cut10',\n",
       " 'F:\\\\PDM\\\\data\\\\cuts\\\\cut11',\n",
       " 'F:\\\\PDM\\\\data\\\\cuts\\\\cut12',\n",
       " 'F:\\\\PDM\\\\data\\\\cuts\\\\cut13',\n",
       " 'F:\\\\PDM\\\\data\\\\cuts\\\\cut14',\n",
       " 'F:\\\\PDM\\\\data\\\\cuts\\\\cut15',\n",
       " 'F:\\\\PDM\\\\data\\\\cuts\\\\cut16',\n",
       " 'F:\\\\PDM\\\\data\\\\cuts\\\\cut17',\n",
       " 'F:\\\\PDM\\\\data\\\\cuts\\\\cut18',\n",
       " 'F:\\\\PDM\\\\data\\\\cuts\\\\cut19',\n",
       " 'F:\\\\PDM\\\\data\\\\cuts\\\\cut20',\n",
       " 'F:\\\\PDM\\\\data\\\\cuts\\\\cut21']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_row(data, row, labels=None):\n",
    "    return data.append(pd.Series(row, labels), ignore_index=True)          #ajoute a data le vecteur avec ses axis labels    \n",
    "\n",
    "#ignore_index= true: the resulting axis are labeled 0,1,...,n-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_motion(data, frames_list, scaler):                          # DATA STANDARDIZATION, scaler peut être plusieurs choses différentes (soit faire en sorte que les data soient entre [0,1] ou [-1,1]....\n",
    "    column_list = list(itertools.chain.from_iterable((f + '_x', f + '_y', f + '_z') for f in frames_list))  \n",
    "    data[column_list] = 2 * scaler.fit_transform(data[column_list]) - 1    #fit transform with the chosen scaler, et *2 -1\n",
    "    \n",
    "    #f prend les valeurs de frame_list données dans la fonction\n",
    "    #on obtient f_x, f_y, f_z\n",
    "    #list: donne la liste de tous les noms (sépare pas quand il y a l'opérateur +) donc: ['f1_x','f1_y','f1_z','f2_x','f2_y',...] \n",
    "    #permet d'accéder aux data qui ont les labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frames_list=['1','2','3','4','5']\n",
    "#from_iterable = itertools.chain.from_iterable((f + '_x', f + '_y', f + '_z') for f in frames_list)\n",
    "#print(list(from_iterable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(data, time_vector):\n",
    "    current_time_index = 0\n",
    "    downsampled_data = pd.DataFrame(columns=data.columns)\n",
    "    \n",
    "    for i in range(len(time_vector)):\n",
    "        t = time_vector.iloc[i]             #t prend la ieme valeur du vecteur de temps\n",
    "        start_time = current_time_index     #part de 0, puis part du dernier stop time quand boucle\n",
    "        \n",
    "        while current_time_index < data.shape[0] and data['relative_time'].iloc[current_time_index] < t:   #pour trouver stop time: tant que la valeur a la position de current_index_time dans la colonne relative time des data est plus petite que la ieme valeur du vecteur de temps ET qu'on n'est pas arrivé au bout de la dimension de data\n",
    "            current_time_index = current_time_index + 1\n",
    "        stop_time = current_time_index\n",
    "\n",
    "        average_data = data.iloc[start_time:stop_time, :].mean()         #calcule la moyenne des data  de start time at stop time\n",
    "        downsampled_data = insert_row(downsampled_data, average_data)    #ajoute les lignes  à la suite avec la fonction spécifique \n",
    "    return downsampled_data\n",
    "\n",
    "#PERMET DE CALCULER LES VALEURS POUR N?IMPORTE QUEL VECTEUR DE TEMPS AVEC LES VALEURS MOYENNES CALCULEES SI ON A PLUSIEURS POINTS DE MESURES COMPRIS ENTRE 2 VALEURS DU VECTEUR DE TEMPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_emg(data, emg_list, scaler):\n",
    "    # first take the absolute value of the data\n",
    "    data[emg_list] = data[emg_list].abs()\n",
    "    # then apply a minmax scaler\n",
    "    data[emg_list] = 2 * scaler.fit_transform(data[emg_list]) - 1\n",
    "    \n",
    "    #DATA STANDARDIZATION,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_list = ['blade', 'wrist']\n",
    "optitrack_list = list(itertools.chain.from_iterable((f + '_x', f + '_y', f + '_z',\n",
    "                                                     f + '_qx', f + '_qy', f + '_qz', f + '_qw')\n",
    "                                                    for f in frames_list))                         #donne ['blade_x','blade_y','blade_z','blade_qx...]\n",
    "emg_list = ['emg' + str(i) for i in range(8)]                                                      #emg0, emg1...emg7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries = []\n",
    "\n",
    "scalers = {}\n",
    "scalers['optitrack'] = MinMaxScaler()           #définit les scaler utilisés pour standardiser les mesures\n",
    "scalers['emg'] = MinMaxScaler()                 #MinMaxScaler: X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0)), X_scaled = X_std * (max - min) + min\n",
    "                                                #Transform features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.\n",
    "\n",
    "for folder in exp_folders:\n",
    "    reader.play(folder)\n",
    "    # extract optitrack data and preprocess them\n",
    "    opt_data = reader.data['optitrack'].reset_index(drop=True)               #enlève les index numériques des data\n",
    "    preprocess_motion(opt_data, frames_list, scalers['optitrack'])           #foction preprocess pour standardizer\n",
    "    # downsample emg and preprocess it\n",
    "    emg_data = downsample(reader.data['emg'], opt_data['relative_time'])     #downsample les emg data pouravoir meme sampling time et time vector que les optitrack data !!!!!!!!!\n",
    "    preprocess_emg(emg_data, emg_list, scalers['emg'])                       #standardisation emg\n",
    "    # merge the data\n",
    "    merge_data = pd.concat([opt_data, emg_data.iloc[:, 3:]], axis=1)         #axis=1: permet de garder sur la meme ligne = on ajoute les valeurs en dessous\n",
    "    # store in the list\n",
    "    timeseries.append(merge_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector=np.array([1,2,3,4,5,6,7,8,8,9])\n",
    "#vector.iloc[:, 3:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save as mat file for Nadia's segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_column_list = optitrack_list\n",
    "\n",
    "# drop the data in matlab\n",
    "matdata = {}\n",
    "matdata['timeseries'] = []\n",
    "for t in timeseries:\n",
    "    # drop the index and times columns and convert to numpy array\n",
    "    matdata['timeseries'].append(t[merged_column_list].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savemat(\"segmentation_data.mat\", matdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply 0 velocity segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(timeseries):\n",
    "    frame_list = ['blade', 'wrist']\n",
    "\n",
    "    header = list(itertools.chain.from_iterable((f + '_dx', f + '_dy', f + '_dz', f + '_vel_mag',\n",
    "                                                 f + '_ddx', f + '_ddy', f + '_ddz', f + '_acc_mag')\n",
    "                                                for f in frame_list))\n",
    "\n",
    "    derived_data = pd.DataFrame(columns=header)\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        row_data = []\n",
    "        for f in frame_list:\n",
    "            # compute velocity\n",
    "            if index == 0:\n",
    "                vel = np.zeros(3)\n",
    "            else:\n",
    "                x0 = data.loc[index - 1, [f + '_x', f + '_y', f + '_z']].to_numpy()\n",
    "                x1 = row.loc[[f + '_x', f + '_y', f + '_z']].to_numpy()\n",
    "                dt = row['relative_time'] - data.loc[index - 1, 'relative_time']\n",
    "                vel = (x1 - x0) / dt\n",
    "            vel_mag = np.linalg.norm(vel)\n",
    "            # compute accleration\n",
    "            if index < 2:\n",
    "                acc = np.zeros(3)\n",
    "            else:\n",
    "                x0 = data.loc[index - 2, [f + '_x', f + '_y', f + '_z']].to_numpy()\n",
    "                x1 = data.loc[index - 1, [f + '_x', f + '_y', f + '_z']].to_numpy()\n",
    "                x2 = row.loc[[f + '_x', f + '_y', f + '_z']].to_numpy()\n",
    "                dt = (row['relative_time'] - data.loc[index - 2, 'relative_time']) / 2.\n",
    "                acc = (x2 - 2*x1 + x0) / (dt *  dt)\n",
    "            acc_mag = np.linalg.norm(acc)\n",
    "            frame_vector = np.hstack((vel, vel_mag, acc, acc_mag))\n",
    "            row_data = np.hstack((row_data, frame_vector))\n",
    "        derived_data = insert_row(derived_data, row_data, header)\n",
    "        timeseries[i] = pd.concat([data, derived_data], axis=1)\n",
    "        \n",
    "        \n",
    "        #to segment based on 0 velocity data = cut each time you have 0 velocity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute normed and rolling average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vel_frames = list(itertools.chain.from_iterable((f + '_vel_mag', f + '_acc_mag') for f in frame_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_velocities = []\n",
    "rolling_average_velocities = []\n",
    "\n",
    "for data in timeseries:\n",
    "    vel_acc_data = data.loc[:, vel_frames]\n",
    "    normed_velocities.append((vel_acc_data - vel_acc_data.min()) / (vel_acc_data.max() - vel_acc_data.min()))\n",
    "    rolling_average_velocities.append(normed_velocities[-1].rolling(window=window_size).mean().iloc[window_size-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "normed_velocities[idx].plot.line()\n",
    "rolling_average_velocities[idx].plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
